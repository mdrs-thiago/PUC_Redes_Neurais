{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUC-Rio \n",
    "### Departamento de Engenharia Elétrica \n",
    "\n",
    "\n",
    "### Aula prática - Echo State Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forma mais prática (didática) para a implementação do Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(ESN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Reservoir parameters\n",
    "        self.reservoir = nn.Linear(reservoir_size, reservoir_size, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.initial_state = nn.Parameter(torch.randn(1, reservoir_size), requires_grad=False)\n",
    "        \n",
    "        # Readout parameters\n",
    "        self.readout = nn.Linear(reservoir_size + input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Initialize reservoir state\n",
    "        reservoir_state = self.initial_state.expand(batch_size, -1)\n",
    "        \n",
    "        # Iterate through time steps\n",
    "        for t in range(seq_length):\n",
    "            # Update reservoir state\n",
    "            input_data = torch.cat((x[:, t], reservoir_state), dim=1)\n",
    "            reservoir_state = self.activation(self.reservoir(input_data))\n",
    "        \n",
    "        # Concatenate reservoir state and input\n",
    "        input_data = torch.cat((x[:, -1], reservoir_state), dim=1)\n",
    "        \n",
    "        # Compute output\n",
    "        output = self.readout(input_data)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação do Echo State Network baseado em https://github.com/stefanonardo/pytorch-esn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta será a versão que usaremos no hands-on, já que possui diversas técnicas e hiperparâmetros utilizados. :-) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que em um Echo State Network, nós temos que assegurar alguns critérios:\n",
    "\n",
    "- Os neurônios do reservatório devem ser esparsamente conectados. \n",
    "\n",
    "- Deve ser satisfeita a propriedade de Echo State. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This examples is not intended to be optimized. Its purpose is to show how to handle\n",
    "big datasets with multiple sequences. The accuracy should be around 10%.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "import torch.sparse\n",
    "\n",
    "\n",
    "def apply_permutation(tensor, permutation, dim=1):\n",
    "    # type: (Tensor, Tensor, int) -> Tensor\n",
    "    return tensor.index_select(dim, permutation)\n",
    "\n",
    "\n",
    "class Reservoir(nn.Module):\n",
    "\n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, leaking_rate,\n",
    "                 spectral_radius, w_ih_scale,\n",
    "                 density, bias=True, batch_first=False):\n",
    "        super(Reservoir, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.w_ih_scale = w_ih_scale\n",
    "        self.density = density\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            layer_input_size = input_size if layer == 0 else hidden_size\n",
    "\n",
    "            w_ih = nn.Parameter(torch.Tensor(hidden_size, layer_input_size))\n",
    "            w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "            b_ih = nn.Parameter(torch.Tensor(hidden_size))\n",
    "            layer_params = (w_ih, w_hh, b_ih)\n",
    "\n",
    "            param_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']\n",
    "            if bias:\n",
    "                param_names += ['bias_ih_l{}{}']\n",
    "            param_names = [x.format(layer, '') for x in param_names]\n",
    "\n",
    "            for name, param in zip(param_names, layer_params):\n",
    "                setattr(self, name, param)\n",
    "            self._all_weights.append(param_names)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(Reservoir, self)._apply(fn)\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        weight_dict = self.state_dict()\n",
    "        for key, value in weight_dict.items():\n",
    "            if key == 'weight_ih_l0':\n",
    "                nn.init.uniform_(value, -1, 1)\n",
    "                value *= self.w_ih_scale[1:]\n",
    "            elif re.fullmatch('weight_ih_l[^0]*', key):\n",
    "                nn.init.uniform_(value, -1, 1)\n",
    "            elif re.fullmatch('bias_ih_l[0-9]*', key):\n",
    "                nn.init.uniform_(value, -1, 1)\n",
    "                value *= self.w_ih_scale[0]\n",
    "            elif re.fullmatch('weight_hh_l[0-9]*', key):\n",
    "                w_hh = torch.Tensor(self.hidden_size * self.hidden_size)\n",
    "                w_hh.uniform_(-1, 1)\n",
    "                if self.density < 1:\n",
    "                    zero_weights = torch.randperm(\n",
    "                        int(self.hidden_size * self.hidden_size))\n",
    "                    zero_weights = zero_weights[\n",
    "                                   :int(\n",
    "                                       self.hidden_size * self.hidden_size * (\n",
    "                                                   1 - self.density))]\n",
    "                    w_hh[zero_weights] = 0\n",
    "                w_hh = w_hh.view(self.hidden_size, self.hidden_size)\n",
    "                abs_eigs = torch.abs(torch.linalg.eigvals(w_hh))\n",
    "                weight_dict[key] = w_hh * (self.spectral_radius / torch.max(abs_eigs))\n",
    "\n",
    "        self.load_state_dict(weight_dict)\n",
    "\n",
    "    def check_input(self, input, batch_sizes):\n",
    "        # type: (Tensor, Optional[Tensor]) -> None\n",
    "        expected_input_dim = 2 if batch_sizes is not None else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "    def get_expected_hidden_size(self, input, batch_sizes):\n",
    "        # type: (Tensor, Optional[Tensor]) -> Tuple[int, int, int]\n",
    "        if batch_sizes is not None:\n",
    "            mini_batch = batch_sizes[0]\n",
    "            mini_batch = int(mini_batch)\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "        expected_hidden_size = (self.num_layers, mini_batch, self.hidden_size)\n",
    "        return expected_hidden_size\n",
    "\n",
    "    def check_hidden_size(self, hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "        # type: (Tensor, Tuple[int, int, int], str) -> None\n",
    "        if hx.size() != expected_hidden_size:\n",
    "            raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        # type: (Tensor, Tensor, Optional[Tensor]) -> None\n",
    "        self.check_input(input, batch_sizes)\n",
    "        expected_hidden_size = self.get_expected_hidden_size(input, batch_sizes)\n",
    "\n",
    "        self.check_hidden_size(hidden, expected_hidden_size)\n",
    "\n",
    "    def permute_hidden(self, hx, permutation):\n",
    "        # type: (Tensor, Optional[Tensor]) -> Tensor\n",
    "        if permutation is None:\n",
    "            return hx\n",
    "        return apply_permutation(hx, permutation)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        is_packed = isinstance(input, PackedSequence)\n",
    "        if is_packed:\n",
    "            input, batch_sizes, sorted_indices, unsorted_indices = input\n",
    "            max_batch_size = int(batch_sizes[0])\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "            sorted_indices = None\n",
    "            unsorted_indices = None\n",
    "\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(self.num_layers, max_batch_size,\n",
    "                                 self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            # Each batch of the hidden state should match the input sequence that\n",
    "            # the user believes he/she is passing in.\n",
    "            hx = self.permute_hidden(hx, sorted_indices)\n",
    "\n",
    "        flat_weight = None\n",
    "\n",
    "        self.check_forward_args(input, hx, batch_sizes)\n",
    "        func = AutogradReservoir(\n",
    "            self.mode,\n",
    "            self.input_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=self.batch_first,\n",
    "            train=self.training,\n",
    "            variable_length=is_packed,\n",
    "            flat_weight=flat_weight,\n",
    "            leaking_rate=self.leaking_rate\n",
    "        )\n",
    "        output, hidden = func(input, self.all_weights, hx, batch_sizes)\n",
    "        if is_packed:\n",
    "            output = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n",
    "        return output, self.permute_hidden(hidden, unsorted_indices)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '({input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        s += ')'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        super(Reservoir, self).__setstate__(d)\n",
    "        self.__dict__.setdefault('_data_ptrs', [])\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            weights = ['weight_ih_l{}{}', 'weight_hh_l{}{}', 'bias_ih_l{}{}']\n",
    "            weights = [x.format(layer) for x in weights]\n",
    "            if self.bias:\n",
    "                self._all_weights += [weights]\n",
    "            else:\n",
    "                self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in\n",
    "                self._all_weights]\n",
    "\n",
    "\n",
    "def AutogradReservoir(mode, input_size, hidden_size, num_layers=1,\n",
    "                      batch_first=False, train=True,\n",
    "                      batch_sizes=None, variable_length=False, flat_weight=None,\n",
    "                      leaking_rate=1):\n",
    "    if mode == 'RES_TANH':\n",
    "        cell = ResTanhCell\n",
    "    elif mode == 'RES_RELU':\n",
    "        cell = ResReLUCell\n",
    "    elif mode == 'RES_ID':\n",
    "        cell = ResIdCell\n",
    "\n",
    "    if variable_length:\n",
    "        layer = (VariableRecurrent(cell, leaking_rate),)\n",
    "    else:\n",
    "        layer = (Recurrent(cell, leaking_rate),)\n",
    "\n",
    "    func = StackedRNN(layer,\n",
    "                      num_layers,\n",
    "                      False,\n",
    "                      train=train)\n",
    "\n",
    "    def forward(input, weight, hidden, batch_sizes):\n",
    "        if batch_first and batch_sizes is None:\n",
    "            input = input.transpose(0, 1)\n",
    "\n",
    "        nexth, output = func(input, hidden, weight, batch_sizes)\n",
    "\n",
    "        if batch_first and not variable_length:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        return output, nexth\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def Recurrent(inner, leaking_rate):\n",
    "    def forward(input, hidden, weight, batch_sizes):\n",
    "        output = []\n",
    "        steps = range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = inner(input[i], hidden, leaking_rate, *weight)\n",
    "            # hack to handle LSTM\n",
    "            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n",
    "\n",
    "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
    "\n",
    "        return hidden, output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def VariableRecurrent(inner, leaking_rate):\n",
    "    def forward(input, hidden, weight, batch_sizes):\n",
    "        output = []\n",
    "        input_offset = 0\n",
    "        last_batch_size = batch_sizes[0]\n",
    "        hiddens = []\n",
    "        flat_hidden = not isinstance(hidden, tuple)\n",
    "        if flat_hidden:\n",
    "            hidden = (hidden,)\n",
    "        for batch_size in batch_sizes:\n",
    "            step_input = input[input_offset:input_offset + batch_size]\n",
    "            input_offset += batch_size\n",
    "\n",
    "            dec = last_batch_size - batch_size\n",
    "            if dec > 0:\n",
    "                hiddens.append(tuple(h[-dec:] for h in hidden))\n",
    "                hidden = tuple(h[:-dec] for h in hidden)\n",
    "            last_batch_size = batch_size\n",
    "\n",
    "            if flat_hidden:\n",
    "                hidden = (inner(step_input, hidden[0], leaking_rate, *weight),)\n",
    "            else:\n",
    "                hidden = inner(step_input, hidden, leaking_rate, *weight)\n",
    "\n",
    "            output.append(hidden[0])\n",
    "        hiddens.append(hidden)\n",
    "        hiddens.reverse()\n",
    "\n",
    "        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n",
    "        assert hidden[0].size(0) == batch_sizes[0]\n",
    "        if flat_hidden:\n",
    "            hidden = hidden[0]\n",
    "        output = torch.cat(output, 0)\n",
    "\n",
    "        return hidden, output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def StackedRNN(inners, num_layers, lstm=False, train=True):\n",
    "    num_directions = len(inners)\n",
    "    total_layers = num_layers * num_directions\n",
    "\n",
    "    def forward(input, hidden, weight, batch_sizes):\n",
    "        assert (len(weight) == total_layers)\n",
    "        next_hidden = []\n",
    "        all_layers_output = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            all_output = []\n",
    "            for j, inner in enumerate(inners):\n",
    "                l = i * num_directions + j\n",
    "\n",
    "                hy, output = inner(input, hidden[l], weight[l], batch_sizes)\n",
    "                next_hidden.append(hy)\n",
    "                all_output.append(output)\n",
    "\n",
    "            input = torch.cat(all_output, input.dim() - 1)\n",
    "            all_layers_output.append(input)\n",
    "\n",
    "        all_layers_output = torch.cat(all_layers_output, -1)\n",
    "        next_hidden = torch.cat(next_hidden, 0).view(\n",
    "            total_layers, *next_hidden[0].size())\n",
    "\n",
    "        return next_hidden, all_layers_output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def ResTanhCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n",
    "    hy_ = torch.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh))\n",
    "    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n",
    "    return hy\n",
    "\n",
    "\n",
    "def ResReLUCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n",
    "    hy_ = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh))\n",
    "    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n",
    "    return hy\n",
    "\n",
    "\n",
    "def ResIdCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n",
    "    hy_ = F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh)\n",
    "    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n",
    "    return hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def prepare_target(target, seq_lengths, washout, batch_first=False):\n",
    "    \"\"\" Preprocess target for offline training.\n",
    "\n",
    "    Args:\n",
    "        target (seq_len, batch, output_size): tensor containing\n",
    "            the features of the target sequence.\n",
    "        seq_lengths: list of lengths of each sequence in the batch.\n",
    "        washout: number of initial timesteps during which output of the\n",
    "            reservoir is not forwarded to the readout. One value per sample.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``\n",
    "\n",
    "    Returns:\n",
    "        tensor containing the features of the batch's sequences rolled out along\n",
    "        one axis, minus the washouts and the padded values.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_first:\n",
    "        target = target.transpose(0, 1)\n",
    "    n_sequences = target.size(1)\n",
    "    target_dim = target.size(2)\n",
    "    train_len = sum(torch.tensor(seq_lengths) - torch.tensor(washout)).item()\n",
    "\n",
    "    new_target = torch.zeros(train_len, target_dim, device=target.device)\n",
    "\n",
    "    idx = 0\n",
    "    for s in range(n_sequences):\n",
    "        batch_len = seq_lengths[s] - washout[s]\n",
    "        new_target[idx:idx + batch_len, :] = target[washout[s]:seq_lengths[s], s, :]\n",
    "        idx += batch_len\n",
    "\n",
    "    return new_target\n",
    "\n",
    "\n",
    "def washout_tensor(tensor, washout, seq_lengths, bidirectional=False, batch_first=False):\n",
    "    tensor = tensor.transpose(0, 1) if batch_first else tensor.clone()\n",
    "    if type(seq_lengths) == list:\n",
    "        seq_lengths = seq_lengths.copy()\n",
    "    if type(seq_lengths) == torch.Tensor:\n",
    "        seq_lengths = seq_lengths.clone()\n",
    "\n",
    "    for b in range(tensor.size(1)):\n",
    "        if washout[b] > 0:\n",
    "            tmp = tensor[washout[b]:seq_lengths[b], b].clone()\n",
    "            tensor[:seq_lengths[b] - washout[b], b] = tmp\n",
    "            tensor[seq_lengths[b] - washout[b]:, b] = 0\n",
    "            seq_lengths[b] -= washout[b]\n",
    "\n",
    "            if bidirectional:\n",
    "                tensor[seq_lengths[b] - washout[b]:, b] = 0\n",
    "                seq_lengths[b] -= washout[b]\n",
    "\n",
    "    if type(seq_lengths) == list:\n",
    "        max_len = max(seq_lengths)\n",
    "    else:\n",
    "        max_len = max(seq_lengths).item()\n",
    "\n",
    "    return tensor[:max_len], seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence\n",
    "#from .reservoir import Reservoir\n",
    "#from ..utils import washout_tensor\n",
    "\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    \"\"\" Applies an Echo State Network to an input sequence. Multi-layer Echo\n",
    "    State Network is based on paper\n",
    "    Deep Echo State Network (DeepESN): A Brief Survey - Gallicchio, Micheli 2017\n",
    "\n",
    "    Args:\n",
    "        input_size: The number of expected features in the input x.\n",
    "        hidden_size: The number of features in the hidden state h.\n",
    "        output_size: The number of expected features in the output y.\n",
    "        num_layers: Number of recurrent layers. Default: 1\n",
    "        nonlinearity: The non-linearity to use ['tanh'|'relu'|'id'].\n",
    "            Default: 'tanh'\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``\n",
    "        leaking_rate: Leaking rate of reservoir's neurons. Default: 1\n",
    "        spectral_radius: Desired spectral radius of recurrent weight matrix.\n",
    "            Default: 0.9\n",
    "        w_ih_scale: Scale factor for first layer's input weights (w_ih_l0). It\n",
    "            can be a number or a tensor of size '1 + input_size' and first element\n",
    "            is the bias' scale factor. Default: 1\n",
    "        lambda_reg: Ridge regression's shrinkage parameter. Default: 1\n",
    "        density: Recurrent weight matrix's density. Default: 1\n",
    "        w_io: If 'True', then the network uses trainable input-to-output\n",
    "            connections. Default: ``False``\n",
    "        readout_training: Readout's traning algorithm ['gd'|'svd'|'cholesky'|'inv'].\n",
    "            If 'gd', gradients are accumulated during backward\n",
    "            pass. If 'svd', 'cholesky' or 'inv', the network will learn readout's\n",
    "            parameters during the forward pass using ridge regression. The\n",
    "            coefficients are computed using SVD, Cholesky decomposition or\n",
    "            standard ridge regression formula. 'gd', 'cholesky' and 'inv'\n",
    "            permit the usage of mini-batches to train the readout.\n",
    "            If 'inv' and matrix is singular, pseudoinverse is used.\n",
    "        output_steps: defines how the reservoir's output will be used by ridge\n",
    "            regression method ['all', 'mean', 'last'].\n",
    "            If 'all', the entire reservoir output matrix will be used.\n",
    "            If 'mean', the mean of reservoir output matrix along the timesteps\n",
    "            dimension will be used.\n",
    "            If 'last', only the last timestep of the reservoir output matrix\n",
    "            will be used.\n",
    "            'mean' and 'last' are useful for classification tasks.\n",
    "\n",
    "    Inputs: input, washout, h_0, target\n",
    "        input (seq_len, batch, input_size): tensor containing the features of\n",
    "            the input sequence. The input can also be a packed variable length\n",
    "            sequence. See `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "        washout (batch): number of initial timesteps during which output of the\n",
    "            reservoir is not forwarded to the readout. One value per batch's\n",
    "            sample.\n",
    "        h_0 (num_layers, batch, hidden_size): tensor containing\n",
    "             the initial reservoir's hidden state for each element in the batch.\n",
    "             Defaults to zero if not provided.\n",
    "\n",
    "        target (seq_len*batch - washout*batch, output_size): tensor containing\n",
    "            the features of the batch's target sequences rolled out along one\n",
    "            axis, minus the washouts and the padded values. It is only needed\n",
    "            for readout's training in offline mode. Use `prepare_target` to\n",
    "            compute it.\n",
    "\n",
    "    Outputs: output, h_n\n",
    "        - output (seq_len, batch, hidden_size): tensor containing the output\n",
    "        features (h_k) from the readout, for each k.\n",
    "        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "          containing the reservoir's hidden state for k=seq_len.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,\n",
    "                 nonlinearity='tanh', batch_first=False, leaking_rate=1,\n",
    "                 spectral_radius=0.9, w_ih_scale=1, lambda_reg=0, density=1,\n",
    "                 w_io=False, readout_training='svd', output_steps='all'):\n",
    "        super(ESN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        if nonlinearity == 'tanh':\n",
    "            mode = 'RES_TANH'\n",
    "        elif nonlinearity == 'relu':\n",
    "            mode = 'RES_RELU'\n",
    "        elif nonlinearity == 'id':\n",
    "            mode = 'RES_ID'\n",
    "        else:\n",
    "            raise ValueError(\"Unknown nonlinearity '{}'\".format(nonlinearity))\n",
    "        self.batch_first = batch_first\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.spectral_radius = spectral_radius\n",
    "        if type(w_ih_scale) != torch.Tensor:\n",
    "            self.w_ih_scale = torch.ones(input_size + 1)\n",
    "            self.w_ih_scale *= w_ih_scale\n",
    "        else:\n",
    "            self.w_ih_scale = w_ih_scale\n",
    "\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.density = density\n",
    "        self.w_io = w_io\n",
    "        if readout_training in {'gd', 'svd', 'cholesky', 'inv'}:\n",
    "            self.readout_training = readout_training\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout training algorithm '{}'\".format(\n",
    "                readout_training))\n",
    "\n",
    "        self.reservoir = Reservoir(mode, input_size, hidden_size, num_layers,\n",
    "                                   leaking_rate, spectral_radius,\n",
    "                                   self.w_ih_scale, density,\n",
    "                                   batch_first=batch_first)\n",
    "\n",
    "        if w_io:\n",
    "            self.readout = nn.Linear(input_size + hidden_size * num_layers,\n",
    "                                     output_size)\n",
    "        else:\n",
    "            self.readout = nn.Linear(hidden_size * num_layers, output_size)\n",
    "        if readout_training == 'offline':\n",
    "            self.readout.weight.requires_grad = False\n",
    "\n",
    "        if output_steps in {'all', 'mean', 'last'}:\n",
    "            self.output_steps = output_steps\n",
    "        else:\n",
    "            raise ValueError(\"Unknown task '{}'\".format(\n",
    "                output_steps))\n",
    "\n",
    "        self.XTX = None\n",
    "        self.XTy = None\n",
    "        self.X = None\n",
    "\n",
    "    def forward(self, input, washout, h_0=None, target=None):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            output, hidden = self.reservoir(input, h_0)\n",
    "\n",
    "            if self.batch_first:\n",
    "                seq_lengths = output.size(0) * [output.size(1)]\n",
    "            else:\n",
    "                seq_lengths = output.size(1) * [output.size(0)]\n",
    "\n",
    "            if self.batch_first:\n",
    "                output = output.transpose(0, 1)\n",
    "\n",
    "            output, seq_lengths = washout_tensor(output, washout, seq_lengths)\n",
    "\n",
    "            if self.w_io:\n",
    "\n",
    "                input_lengths = [input.size(0)] * input.size(1)\n",
    "\n",
    "                if self.batch_first:\n",
    "                    input = input.transpose(0, 1)\n",
    "\n",
    "                input, _ = washout_tensor(input, washout, input_lengths)\n",
    "                output = torch.cat([input, output], -1)\n",
    "\n",
    "            if self.readout_training == 'gd' or target is None:\n",
    "                with torch.enable_grad():\n",
    "                    output = self.readout(output)\n",
    "\n",
    "                    if self.batch_first:\n",
    "                        output = output.transpose(0, 1)\n",
    "\n",
    "                    # Uncomment if you want packed output.\n",
    "                    # if is_packed:\n",
    "                    #     output = pack_padded_sequence(output, seq_lengths,\n",
    "                    #                                   batch_first=self.batch_first)\n",
    "\n",
    "                    return output, hidden\n",
    "\n",
    "            else:\n",
    "                batch_size = output.size(1)\n",
    "\n",
    "                X = torch.ones(target.size(0), 1 + output.size(2), device=target.device)\n",
    "                row = 0\n",
    "                for s in range(batch_size):\n",
    "                    if self.output_steps == 'all':\n",
    "                        X[row:row + seq_lengths[s], 1:] = output[:seq_lengths[s],\n",
    "                                                          s]\n",
    "                        row += seq_lengths[s]\n",
    "                    elif self.output_steps == 'mean':\n",
    "                        X[row, 1:] = torch.mean(output[:seq_lengths[s], s], 0)\n",
    "                        row += 1\n",
    "                    elif self.output_steps == 'last':\n",
    "                        X[row, 1:] = output[seq_lengths[s] - 1, s]\n",
    "                        row += 1\n",
    "\n",
    "                if self.readout_training == 'cholesky':\n",
    "                    if self.XTX is None:\n",
    "                        self.XTX = torch.mm(X.t(), X)\n",
    "                        self.XTy = torch.mm(X.t(), target)\n",
    "                    else:\n",
    "                        self.XTX += torch.mm(X.t(), X)\n",
    "                        self.XTy += torch.mm(X.t(), target)\n",
    "\n",
    "                elif self.readout_training == 'svd':\n",
    "                    # Scikit-Learn SVD solver for ridge regression.\n",
    "                    U, s, V = torch.svd(X)\n",
    "                    idx = s > 1e-15  # same default value as scipy.linalg.pinv\n",
    "                    s_nnz = s[idx][:, None]\n",
    "                    UTy = torch.mm(U.t(), target)\n",
    "                    d = torch.zeros(s.size(0), 1, device=X.device)\n",
    "                    d[idx] = s_nnz / (s_nnz ** 2 + self.lambda_reg)\n",
    "                    d_UT_y = d * UTy\n",
    "                    W = torch.mm(V, d_UT_y).t()\n",
    "\n",
    "                    self.readout.bias = nn.Parameter(W[:, 0])\n",
    "                    self.readout.weight = nn.Parameter(W[:, 1:])\n",
    "                elif self.readout_training == 'inv':\n",
    "                    self.X = X\n",
    "                    if self.XTX is None:\n",
    "                        self.XTX = torch.mm(X.t(), X)\n",
    "                        self.XTy = torch.mm(X.t(), target)\n",
    "                    else:\n",
    "                        self.XTX += torch.mm(X.t(), X)\n",
    "                        self.XTy += torch.mm(X.t(), target)\n",
    "\n",
    "                return None, None\n",
    "\n",
    "    def fit(self):\n",
    "        if self.readout_training in {'gd', 'svd'}:\n",
    "            return\n",
    "\n",
    "        if self.readout_training == 'cholesky':\n",
    "            W = torch.linalg.solve(self.XTy,\n",
    "                                   self.XTX + self.lambda_reg * torch.eye(\n",
    "                                       self.XTX.size(0), device=self.XTX.device))[0].t()\n",
    "            self.XTX = None\n",
    "            self.XTy = None\n",
    "\n",
    "            self.readout.bias = nn.Parameter(W[:, 0])\n",
    "            self.readout.weight = nn.Parameter(W[:, 1:])\n",
    "        elif self.readout_training == 'inv':\n",
    "            I = (self.lambda_reg * torch.eye(self.XTX.size(0))).to(\n",
    "                self.XTX.device)\n",
    "            A = self.XTX + I\n",
    "            X_rank = torch.linalg.matrix_rank(A).item()\n",
    "\n",
    "            if X_rank == self.X.size(0):\n",
    "                W = torch.mm(torch.inverse(A), self.XTy).t()\n",
    "            else:\n",
    "                W = torch.mm(torch.pinverse(A), self.XTy).t()\n",
    "\n",
    "            self.readout.bias = nn.Parameter(W[:, 0])\n",
    "            self.readout.weight = nn.Parameter(W[:, 1:])\n",
    "\n",
    "            self.XTX = None\n",
    "            self.XTy = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reservoir.reset_parameters()\n",
    "        self.readout.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparando os dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o primeiro experimento, vamos utilizar uma ESN para modelar a dinâmica de um dispositivo de CD player, tendo as entradas as forças aplicadas (nos dois eixos, x e y) pelo 'braço' do CD, e como saídas as posições (x e y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dtype = torch.float\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "filename = 'CD_player_arm.dat'\n",
    "\n",
    "data = np.loadtxt(filename, dtype=np.float32)\n",
    "\n",
    "X_data = np.expand_dims(data[:, :2], axis=1)\n",
    "Y_data = np.expand_dims(data[:, 2:], axis=1)\n",
    "\n",
    "X_data = torch.from_numpy(X_data)\n",
    "Y_data = torch.from_numpy(Y_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo uma divisão para treinamento e teste. Neste caso, vamos usar os primeiros 80% dos dados para treinamento, e o restante para o <b> teste </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = int(0.8*X_data.shape[0])\n",
    "\n",
    "trX = X_data[:s]\n",
    "trY = Y_data[:s]\n",
    "tsX = X_data[s:]\n",
    "tsY = Y_data[s:]\n",
    "tsY = tsY.reshape(tsY.shape[0], tsY.shape[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que, para o ESN, temos o tempo de washout. Isso significa que vamos esperar um determinado período para 'passar' a condição inicial aleatória e começarmos a computar as saídas do reservatório para o treinamento da matriz de pesos da saída. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "washout = [100]\n",
    "\n",
    "# Training\n",
    "trY_flat = prepare_target(trY.clone(), [trX.size(0)], washout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do treinamento era (1638, 2) e agora é (1538, 2) usando washout = 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensão do treinamento era {trY.shape[0], trY.shape[2]} e agora é {trY_flat.shape[0], trY_flat.shape[1]} usando washout = {washout[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definindo o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = trX.shape[2]\n",
    "output_size = trY.shape[2]\n",
    "hidden_size = 100\n",
    "density = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESN(input_size, hidden_size, output_size, density=density)\n",
    "\n",
    "_ = model(trX, washout, None, trY_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Treinamento da Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 ms ± 6.19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model.fit()\n",
    "output, hidden = model(trX, washout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()\n",
    "output, hidden = model(trX, washout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.008149297907948494\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error:\", loss_fcn(output, trY[washout[0]:]).item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Teste da Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.48554036021232605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([410, 2])) that is different to the input size (torch.Size([410, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "output, hidden = model(tsX, [0], hidden)\n",
    "print(\"Test error:\", loss_fcn(output, tsY).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def root_relative_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Root Relative Squared Error (RRSE).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Array of true values.\n",
    "        y_pred (array-like): Array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: RRSE value.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate the mean squared error (MSE)\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    # Calculate the squared difference between the actual values and the mean of the actual values\n",
    "    mean_actual = np.mean(y_true)\n",
    "    squared_difference = np.mean((y_true - mean_actual) ** 2)\n",
    "\n",
    "    # Calculate the root relative squared error (RRSE)\n",
    "    rrse = np.sqrt(mse / squared_difference)\n",
    "\n",
    "    return rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.detach()\n",
    "output = torch.squeeze(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrse_var1 = root_relative_squared_error(output[:,0], tsY[:,0])\n",
    "rrse_var2 = root_relative_squared_error(output[:,1], tsY[:,1])\n",
    "\n",
    "mae = np.array(torch.mean(abs(output - tsY),dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.3855954110622406, MAE = 0.0916481763124466 \n",
      "Output #2 - RRSE = 0.1852477788925171, MAE = 0.09282106906175613\n"
     ]
    }
   ],
   "source": [
    "print(f'Output #1 - RRSE = {rrse_var1}, MAE = {mae[0]} \\nOutput #2 - RRSE = {rrse_var2}, MAE = {mae[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que esse modelo, assim como o MLP, depende muito da inicialização. Vamos rodar algumas vezes. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "rrse_v1 = []\n",
    "rrse_v2 = [] \n",
    "mae_ = []\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    model = ESN(input_size, hidden_size, output_size, density=density)\n",
    "\n",
    "    _ = model(trX, washout, None, trY_flat)\n",
    "\n",
    "    model.fit()\n",
    "    output_train, hidden = model(trX, washout)\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    output, hidden = model(tsX, [0], hidden)\n",
    "\n",
    "    loss_train.append(loss_fcn(output_train, trY[washout[0]:]).item())\n",
    "\n",
    "    loss_test.append(loss_fcn(output, tsY).item())\n",
    "\n",
    "    output = torch.squeeze(output.detach(), dim=1)\n",
    "        \n",
    "    rrse_v1.append(root_relative_squared_error(output[:,0], tsY[:,0]))\n",
    "    rrse_v2.append(root_relative_squared_error(output[:,1], tsY[:,1]))\n",
    "\n",
    "    mae_.append(np.array(torch.mean(abs(output - tsY),dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.array(loss_train)\n",
    "loss_test = np.array(loss_test)\n",
    "rrse_v1 = np.array(rrse_v1)\n",
    "rrse_v2 = np.array(rrse_v2)\n",
    "mae_ = np.array(mae_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.4070626199245453 +- 0.017119862139225006, MAE = 0.09466187655925751 +- 0.0033661064226180315 \n",
      "Output #2 - RRSE = 0.197372704744339 +- 0.005639947485178709, MAE = 0.09902842342853546 +- 0.0028421322349458933\n"
     ]
    }
   ],
   "source": [
    "print(f'Output #1 - RRSE = {np.mean(rrse_v1)} +- {np.std(rrse_v1)}, MAE = {np.mean(mae_,axis=0)[0]} +- {np.std(mae_,axis=0)[0]} \\nOutput #2 - RRSE = {np.mean(rrse_v2)} +- {np.std(rrse_v2)}, MAE = {np.mean(mae_,axis=0)[1]} +- {np.std(mae_,axis=0)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09466188, 0.09902842], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mae_,axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mudando hiperparâmetros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mudando a esparsidade das conexões "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.4210814833641052 +- 0.015861159190535545, MAE = 0.09549697488546371 +- 0.0033339334186166525 \n",
      "Output #2 - RRSE = 0.1910683512687683 +- 0.006454088259488344, MAE = 0.09664402902126312 +- 0.003075381275266409\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "rrse_v1 = []\n",
    "rrse_v2 = [] \n",
    "mae_ = []\n",
    "\n",
    "density = 0.6\n",
    "\n",
    "for i in range(n):\n",
    "    model = ESN(input_size, hidden_size, output_size, density=density)\n",
    "\n",
    "    _ = model(trX, washout, None, trY_flat)\n",
    "\n",
    "    model.fit()\n",
    "    output_train, hidden = model(trX, washout)\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    output_, hidden = model(tsX, [0], hidden)\n",
    "\n",
    "    loss_train.append(loss_fcn(output_train, trY[washout[0]:]).item())\n",
    "\n",
    "    loss_test.append(loss_fcn(output_, tsY).item())\n",
    "\n",
    "    output = torch.squeeze(output_.detach(), dim=1)\n",
    "        \n",
    "    rrse_v1.append(root_relative_squared_error(output[:,0], tsY[:,0]))\n",
    "    rrse_v2.append(root_relative_squared_error(output[:,1], tsY[:,1]))\n",
    "\n",
    "    mae_.append(np.array(torch.mean(abs(output - tsY),dim=0)))\n",
    "\n",
    "loss_train = np.array(loss_train)\n",
    "loss_test = np.array(loss_test)\n",
    "rrse_v1 = np.array(rrse_v1)\n",
    "rrse_v2 = np.array(rrse_v2)\n",
    "mae_ = np.array(mae_)\n",
    "\n",
    "print(f'Output #1 - RRSE = {np.mean(rrse_v1)} +- {np.std(rrse_v1)}, MAE = {np.mean(mae_,axis=0)[0]} +- {np.std(mae_,axis=0)[0]} \\nOutput #2 - RRSE = {np.mean(rrse_v2)} +- {np.std(rrse_v2)}, MAE = {np.mean(mae_,axis=0)[1]} +- {np.std(mae_,axis=0)[1]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mudando o Leaking Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.4314011037349701 +- 0.02539672888815403, MAE = 0.09914489090442657 +- 0.0033575049601495266 \n",
      "Output #2 - RRSE = 0.19682073593139648 +- 0.0044317287392914295, MAE = 0.09940026700496674 +- 0.002229820005595684\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "rrse_v1 = []\n",
    "rrse_v2 = [] \n",
    "mae_ = []\n",
    "\n",
    "density = 0.6\n",
    "leaking_rate = 0.8\n",
    "\n",
    "for i in range(n):\n",
    "    model = ESN(input_size, hidden_size, output_size, density=density, leaking_rate=leaking_rate)\n",
    "\n",
    "    _ = model(trX, washout, None, trY_flat)\n",
    "\n",
    "    model.fit()\n",
    "    output_train, hidden = model(trX, washout)\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    output_, hidden = model(tsX, [0], hidden)\n",
    "\n",
    "    loss_train.append(loss_fcn(output_train, trY[washout[0]:]).item())\n",
    "\n",
    "    loss_test.append(loss_fcn(output_, tsY).item())\n",
    "\n",
    "    output = torch.squeeze(output_.detach(), dim=1)\n",
    "        \n",
    "    rrse_v1.append(root_relative_squared_error(output[:,0], tsY[:,0]))\n",
    "    rrse_v2.append(root_relative_squared_error(output[:,1], tsY[:,1]))\n",
    "\n",
    "    mae_.append(np.array(torch.mean(abs(output - tsY),dim=0)))\n",
    "\n",
    "loss_train = np.array(loss_train)\n",
    "loss_test = np.array(loss_test)\n",
    "rrse_v1 = np.array(rrse_v1)\n",
    "rrse_v2 = np.array(rrse_v2)\n",
    "mae_ = np.array(mae_)\n",
    "\n",
    "print(f'Output #1 - RRSE = {np.mean(rrse_v1)} +- {np.std(rrse_v1)}, MAE = {np.mean(mae_,axis=0)[0]} +- {np.std(mae_,axis=0)[0]} \\nOutput #2 - RRSE = {np.mean(rrse_v2)} +- {np.std(rrse_v2)}, MAE = {np.mean(mae_,axis=0)[1]} +- {np.std(mae_,axis=0)[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.41981709003448486 +- 0.02124977670609951, MAE = 0.09687023609876633 +- 0.003925939556211233 \n",
      "Output #2 - RRSE = 0.19524364173412323 +- 0.00787263922393322, MAE = 0.09818563610315323 +- 0.002922265324741602\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "rrse_v1 = []\n",
    "rrse_v2 = [] \n",
    "mae_ = []\n",
    "\n",
    "density = 0.6\n",
    "leaking_rate = 0.95\n",
    "spectral_radius = 0.7\n",
    "\n",
    "for i in range(n):\n",
    "    model = ESN(input_size, hidden_size, output_size, density=density, leaking_rate=leaking_rate, spectral_radius=spectral_radius)\n",
    "\n",
    "    _ = model(trX, washout, None, trY_flat)\n",
    "\n",
    "    model.fit()\n",
    "    output_train, hidden = model(trX, washout)\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    output_, hidden = model(tsX, [0], hidden)\n",
    "\n",
    "    loss_train.append(loss_fcn(output_train, trY[washout[0]:]).item())\n",
    "\n",
    "    loss_test.append(loss_fcn(output_, tsY).item())\n",
    "\n",
    "    output = torch.squeeze(output_.detach(), dim=1)\n",
    "        \n",
    "    rrse_v1.append(root_relative_squared_error(output[:,0], tsY[:,0]))\n",
    "    rrse_v2.append(root_relative_squared_error(output[:,1], tsY[:,1]))\n",
    "\n",
    "    mae_.append(np.array(torch.mean(abs(output - tsY),dim=0)))\n",
    "\n",
    "loss_train = np.array(loss_train)\n",
    "loss_test = np.array(loss_test)\n",
    "rrse_v1 = np.array(rrse_v1)\n",
    "rrse_v2 = np.array(rrse_v2)\n",
    "mae_ = np.array(mae_)\n",
    "\n",
    "print(f'Output #1 - RRSE = {np.mean(rrse_v1)} +- {np.std(rrse_v1)}, MAE = {np.mean(mae_,axis=0)[0]} +- {np.std(mae_,axis=0)[0]} \\nOutput #2 - RRSE = {np.mean(rrse_v2)} +- {np.std(rrse_v2)}, MAE = {np.mean(mae_,axis=0)[1]} +- {np.std(mae_,axis=0)[1]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos testar o mesmo modelo em diferentes problemas para identificação de sistemas. Por exemplo, vamos aplicar ESN em outro benchmark, disponível em https://homes.esat.kuleuven.be/~smc/daisy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dtype = torch.float\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "filename = 'cstr.dat'\n",
    "\n",
    "data = np.loadtxt(filename, dtype=np.float32)\n",
    "\n",
    "X_data = np.expand_dims(data[:, :2], axis=1)\n",
    "Y_data = np.expand_dims(data[:, 2:], axis=1)\n",
    "\n",
    "X_data = torch.from_numpy(X_data)\n",
    "Y_data = torch.from_numpy(Y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thiag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([410, 2])) that is different to the input size (torch.Size([410, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output #1 - RRSE = 0.4295271635055542 +- 0.026224739849567413, MAE = 0.0987132117152214 +- 0.0029563955031335354 \n",
      "Output #2 - RRSE = 0.1988455206155777 +- 0.006065335590392351, MAE = 0.09996839612722397 +- 0.003667193464934826\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "rrse_v1 = []\n",
    "rrse_v2 = [] \n",
    "mae_ = []\n",
    "\n",
    "density = 0.6\n",
    "leaking_rate = 0.8\n",
    "\n",
    "for i in range(n):\n",
    "    model = ESN(input_size, hidden_size, output_size, density=density, leaking_rate=leaking_rate)\n",
    "\n",
    "    _ = model(trX, washout, None, trY_flat)\n",
    "\n",
    "    model.fit()\n",
    "    output_train, hidden = model(trX, washout)\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    output_, hidden = model(tsX, [0], hidden)\n",
    "\n",
    "    loss_train.append(loss_fcn(output_train, trY[washout[0]:]).item())\n",
    "\n",
    "    loss_test.append(loss_fcn(output_, tsY).item())\n",
    "\n",
    "    output = torch.squeeze(output_.detach(), dim=1)\n",
    "        \n",
    "    rrse_v1.append(root_relative_squared_error(output[:,0], tsY[:,0]))\n",
    "    rrse_v2.append(root_relative_squared_error(output[:,1], tsY[:,1]))\n",
    "\n",
    "    mae_.append(np.array(torch.mean(abs(output - tsY),dim=0)))\n",
    "\n",
    "loss_train = np.array(loss_train)\n",
    "loss_test = np.array(loss_test)\n",
    "rrse_v1 = np.array(rrse_v1)\n",
    "rrse_v2 = np.array(rrse_v2)\n",
    "mae_ = np.array(mae_)\n",
    "\n",
    "print(f'Output #1 - RRSE = {np.mean(rrse_v1)} +- {np.std(rrse_v1)}, MAE = {np.mean(mae_,axis=0)[0]} +- {np.std(mae_,axis=0)[0]} \\nOutput #2 - RRSE = {np.mean(rrse_v2)} +- {np.std(rrse_v2)}, MAE = {np.mean(mae_,axis=0)[1]} +- {np.std(mae_,axis=0)[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
